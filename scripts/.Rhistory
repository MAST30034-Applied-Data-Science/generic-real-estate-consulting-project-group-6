pf(Fstat, m, n-r, lower=F)
Fstat
C = matrix(c(0,1,-1,0,0,0,1,-1), nrow = 2, ncol=4)
m = 2
Fstat = t(C%*%b) %*% solve(C %*% XtXc %*% t(C)) %*% (C%*%b) / (m * s2)
Fstat
t.new = matrix(data = c(0,0,-1,1), nrow = 4, ncol = 1)
Tstat = (t(t.new) %*% b - 2) / sqrt(s2 * t(t.new) %*% XtXc %*% t.new)
Tstat
qt(0.05, 33)
pt(Tstat, n-r, lower.tail)
pt(Tstat, n-r)
qt(0.95, 33)
pt(Tstat, n-r, lower.tail = TRUE)
p_val = pt(Tstat, n-r, lower.tail = TRUE)
p_val
data = read.csv("heli.csv")
data
data = read.csv("heli.csv")
str(data)
plot(data$Load, data$Velocity, pch = as.character(data$Tree))
plot(data$Load, data$Velocity, pch = as.character(data$Tree), col = data$Tree + 2)
plot(data$Load, data$Velocity, pch = as.character(data$Tree), col = data$Tree + 1)
model.add <- lm(data.y ~ Load + Tree)
model.add <- lm(data$Velocity ~ Load + Tree)
model.add <- lm(data.y ~ Load + Tree, data = data)
model.add <- lm(data.Velocity ~ Load + Tree, data = data)
model.add <- lm(Velocity ~ Load + Tree, data = data)
summary(model.add)
data.h = read.csv("heli.csv")
plot(data.h$Load, data.h$Velocity, pch = as.character(data.h$Tree), col = data.h$Tree + 1)
model.add <- lm(Velocity ~ Load + Tree, data = data.h)
summary(model.add)
model.add <- lm(Velocity ~ Load + Tree, data = data.h)
model.int <- lm(Velocity ~ Load * Tree, data = data.h)
summary(model.int)
model.add <- lm(Velocity ~ Load + Tree, data = data.h)
model.int <- lm(Velocity ~ Load * Tree, data = data.h)
anova(model.add, model.int)
data.h$Tree <- factor(data.h$Tree)
model.add <- lm(Velocity ~ Load + Tree, data = data.h)
model.int <- lm(Velocity ~ Load * Tree, data = data.h)
anova(model.add, model.int)
drop1(model.int, scope = ~, test = 'F')
drop1(model.int, scope = ~., test = 'F')
drop1(model.int, scope = ~., test = 'AIC')
drop1(model.int, scope = ~., test = 'F')
drop1(model.int, scope = ~., test = 'F')
model2 <- lm(Velocity ~ Load + Tree, data = data.h)
drop1(model.int, scope = ~., test = 'F')
model2 <- lm(Velocity ~ Load + Tree, data = data.h)
drop1(model2, scope = ~., test = 'F)
drop1(model.int, scope = ~., test = 'F')
model2 <- lm(Velocity ~ Load + Tree, data = data.h)
drop1(model2, scope = ~., test = 'F')
drop1(model.int, scope = ~., test = 'F')
model2 <- lm(Velocity ~ Load + Tree, data = data.h)
drop1(model2, scope = ~., test = 'F')
model3 <- lm(Velocity ~ Load, data = data.h)
drop1(model.int, scope = ~., test = 'F')
model2 <- lm(Velocity ~ Load + Tree, data = data.h)
drop1(model2, scope = ~., test = 'F')
model3 <- lm(Velocity ~ Load, data = data.h)
drop1(model3, scope = ~., test = 'F')
with(
math,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(class) + 1)
)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1)
)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
lines(data.h$Load, fitted(model3))
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
lines(data.h$Load, fitted(model3))
lines(data.h$Load, fitted(model.int), col = as.numeric(data.h$Tree) + 1)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
lines(data.h$Load, fitted(model3))
lines(data.h$Load[as.numeric(data.h$Tree) == 1], fitted(model.int), col = as.numeric(data.h$Tree) + 1)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
wich(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
with(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
with(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
lines(data.h$Load, fitted(model3))
lines(data.h$Load[as.numeric(data.h$Tree) == 1], fitted(model.int), col = as.numeric(data.h$Tree) + 1)
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
with(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
lines(data.h$Load, fitted(model3))
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
with(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
with(
data.h,
lines(Load[as.numeric(Tree) == 2], fitted(model.int)[as.numeric(Tree) == 2], col = 3)
)
lines(data.h$Load, fitted(model3))
with(
data.h,
plot(Load, Velocity, pch = as.character(Tree), col = as.numeric(Tree) + 1),
)
with(
data.h,
lines(Load[as.numeric(Tree) == 1], fitted(model.int)[as.numeric(Tree) == 1], col = 2)
)
with(
data.h,
lines(Load[as.numeric(Tree) == 2], fitted(model.int)[as.numeric(Tree) == 2], col = 3)
)
with(
data.h,
lines(Load[as.numeric(Tree) == 3], fitted(model.int)[as.numeric(Tree) == 3], col = 4)
)
lines(data.h$Load, fitted(model3))
summary(model.int)
summary(model.int)
library(car)
linearHypothesis( model.int,
c(1, 0.2, 1, 0, 0.2, 0), 1, test = 'F'
)
summary(model.int)
library(car)
linearHypothesis( model.int,
c(1, 0.2, 1, 0, 0.2, 0), 1, test = 'F'
)
knitr::opts_chunk$set(echo = TRUE)
## Load data
library(faraway)
install.packages("lme4")
library(faraway)
library(lme4)
install.packages("Rcpp")
install.packages("lme4")
pchi((3124962.2053 - 2702579.4574), 579)
pchisq((3124962.2053 - 2702579.4574), 579)
pchisq((3124962.2053 - 2702579.4574) > 0, 579)
qchisq(0.95, 579)
pchisq((3124962.2053 - 2702579.4574), 579, lower.tail = FALSE)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/MAS/A3")
all(diff(EM2.more$log_liks) > 0)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/MAS/A3")
X = scan(file="assignment3_prob1.txt", what=double())
length(X)
hist(X)
# pi.init : initial value for pi
# p.init : initial value for p
# epsilon : If the incomplete log-likelihood has changed by less than epsilon,
# EM will stop.
# max.iter : maximum number of EM-iterations
mixture.EM <- function(X, pi.init, p.init, epsilon=1e-5, max.iter=100) {
pi.curr = pi.init
p.curr = p.init
# store incomplete log-likehoods for each iteration
log_liks = c()
# compute incomplete log-likehoods using initial values of parameters.
log_liks = c(log_liks, compute.log.lik(X, pi.curr, p.curr)$ill)
# set the change in incomplete log-likelihood with 1
delta.ll = 1
# number of iteration
n.iter = 1
# If the log-likelihood has changed by less than epsilon, EM will stop.
while((delta.ll > epsilon) & (n.iter <= max.iter)){
# run EM step
EM.out = EM.iter(X, pi.curr, p.curr)
# replace the current value with the new parameter estimate
pi.curr = EM.out$pi.new
p.curr = EM.out$p.new
# incomplete log-likehoods with new parameter estimate
log_liks = c(log_liks, compute.log.lik(X, pi.curr, p.curr)$ill)
# compute the change in incomplete log-likelihood
delta.ll = log_liks[length(log_liks)]  - log_liks[length(log_liks)-1]
# increase the number of iteration
n.iter = n.iter + 1
}
return(list(pi.curr=pi.curr, p.curr=p.curr, log_liks=log_liks))
}
EM.iter <- function(X, pi.curr, p.curr) {
# E-step: compute E_{Z|X,\theta_0}[I(Z_i = k)]
# for each sample $X_i$, compute $P(X_i, Z_i=k)$
prob.x.z = compute.prob.x.z(X, pi.curr, p.curr)$prob.x.z
# compute P(Z_i=k | X_i)
P_ik = prob.x.z / rowSums(prob.x.z)
# M-step
pi.new = colSums(P_ik)/sum(P_ik)  # sum(P_ik) is equivalent to sample size
p.new = colSums(P_ik*X)/colSums(20*P_ik)
return(list(pi.new=pi.new, p.new=p.new))
}
# Compute incomplete log-likehoods
compute.log.lik <- function(X, pi.curr, p.curr) {
# for each sample $X_i$, compute $P(X_i, Z_i=k)$
prob.x.z = compute.prob.x.z(X, pi.curr, p.curr)$prob.x.z
# incomplete log-likehoods
ill = sum(log(rowSums(prob.x.z)))
return(list(ill=ill))
}
# for each sample $X_i$, compute $P(X_i, Z_i=kEM@)$
compute.prob.x.z <- function(X, pi.curr, p.curr) {
# for each sample $X_i$, compute $P(X_i, Z_i=k)$. Store these values in the columns of L:
L = matrix(NA, nrow=length(X), ncol= length(p.curr))
for(k in seq_len(ncol(L))) {
L[, k] = pi.curr[k] * dbinom(X, size=20, prob=p.curr[k])
}
return(list(prob.x.z=L))
}
pi.init1 <- c(0.3,0.3,0.4)
p.init1 <- c(0.2,0.5,0.7)
pi.init2 <- c(0.1,0.2,0.7)
p.init2 <- c(0.1,0.3,0.7)
EM1 <- mixture.EM(X, pi.init1, p.init1)
EM2 <- mixture.EM(X, pi.init2, p.init2)
print(paste("Estimate pi = (", round(EM1$pi.curr[1],4), ",",
round(EM1$pi.curr[2],4), ",",
round(EM1$pi.curr[3],4), ")", sep=""))
print(paste("Estimate p = (", round(EM1$p.curr[1],4), ",",
round(EM1$p.curr[2],4), ",",
round(EM1$p.curr[3],4), ")", sep=""))
EM1$log_liks[length(EM1$log_liks)]
all(diff(EM1$log_liks) > 0)
plot(EM1$log_liks, ylab='incomplete log-likelihood', xlab='iteration')
print(paste("Estimate pi = (", round(EM2$pi.curr[1],4), ",",
round(EM2$pi.curr[2],4), ",",
round(EM2$pi.curr[3],4), ")", sep=""))
print(paste("Estimate p = (", round(EM2$p.curr[1],4), ",",
round(EM2$p.curr[2],4), ",",
round(EM2$p.curr[3],4), ")", sep=""))
EM2$log_liks[length(EM2$log_liks)]
all(diff(EM2$log_liks) > 0)
plot(EM2$log_liks, ylab='incomplete log-likelihood', xlab='iteration')
X.more = scan(file="assignment3_prob2.txt", what=double())
length(X)
length(X.more)
par(mfrow=c(2,2))
hist(X, xlim=c(0,20), ylim=c(0,80))
hist(X.more, xlim=c(0,20), ylim=c(0,80))
hist(c(X,X.more), xlim=c(0,20), ylim=c(0,80), xlab="X + X.more", main = "Histogram of X + X.more")
# Compute incomplete log-likehoods
compute.log.lik.more <- function(X, X.more, pi.curr, p.curr) {
# for each sample $X_i$, compute $P(X_i, Z_i=k)$
prob.x.z = compute.prob.x.z(X, pi.curr, p.curr)$prob.x.z
# Extra chuck
extra = sum(log(dbinom(X.more, size=20, prob=p.curr[1])))
# incomplete log-likehoods
ill = sum(log(rowSums(prob.x.z))) + extra
return(list(ill=ill))
}
EM.iter.more <- function(X, X.more, pi.curr, p.curr) {
# E-step: compute E_{Z|X,\theta_0}[I(Z_i = k)]
# for each sample $X_i$, compute $P(X_i, Z_i=k)$
prob.x.z = compute.prob.x.z(X, pi.curr, p.curr)$prob.x.z
# compute P(Z_i=k | X_i)
P_ik = prob.x.z / rowSums(prob.x.z)
# M-step
pi.new = colSums(P_ik)/sum(P_ik)  # sum(P_ik) is equivalent to sample size
p.new = colSums(P_ik*X)/colSums(20*P_ik)
p.new[1] = (colSums(P_ik*X)[1] + sum(X.more)) / (colSums(20*P_ik)[1] + 20*length(X.more))
return(list(pi.new=pi.new, p.new=p.new))
}
# pi.init : initial value for pi
# p.init : initial value for p
# epsilon : If the incomplete log-likelihood has changed by less than epsilon,
# EM will stop.
# max.iter : maximum number of EM-iterations
mixture.EM.more <- function(X, X.more, pi.init, p.init, epsilon=1e-5, max.iter=100) {
pi.curr = pi.init
p.curr = p.init
# store incomplete log-likehoods for each iteration
log_liks = c()
# compute incomplete log-likehoods using initial values of parameters.
log_liks = c(log_liks, compute.log.lik.more(X, X.more, pi.curr, p.curr)$ill)
# set the change in incomplete log-likelihood with 1
delta.ll = 1
# number of iteration
n.iter = 1
# If the log-likelihood has changed by less than epsilon, EM will stop.
while((delta.ll > epsilon) & (n.iter <= max.iter)){
# run EM step
EM.out = EM.iter.more(X, X.more, pi.curr, p.curr)
# replace the current value with the new parameter estimate
pi.curr = EM.out$pi.new
p.curr = EM.out$p.new
# incomplete log-likehoods with new parameter estimate
log_liks = c(log_liks, compute.log.lik.more(X, X.more, pi.curr, p.curr)$ill)
# compute the change in incomplete log-likelihood
delta.ll = log_liks[length(log_liks)]  - log_liks[length(log_liks)-1]
# increase the number of iteration
n.iter = n.iter + 1
}
return(list(pi.curr=pi.curr, p.curr=p.curr, log_liks=log_liks))
}
EM1.more <- mixture.EM.more(X, X.more, pi.init1, p.init1)
EM2.more <- mixture.EM.more(X, X.more, pi.init2, p.init2)
print(paste("Estimate pi = (", round(EM1.more$pi.curr[1],4), ",",
round(EM1.more$pi.curr[2],4), ",",
round(EM1.more$pi.curr[3],4), ")", sep=""))
print(paste("Estimate p = (", round(EM1.more$p.curr[1],4), ",",
round(EM1.more$p.curr[2],4), ",",
round(EM1.more$p.curr[3],4), ")", sep=""))
EM1.more$log_liks[length(EM1.more$log_liks)]
all(diff(EM1.more$log_liks) > 0)
plot(EM1.more$log_liks, ylab='incomplete log-likelihood', xlab='iteration')
print(paste("Estimate pi = (", round(EM2.more$pi.curr[1],4), ",",
round(EM2.more$pi.curr[2],4), ",",
round(EM2.more$pi.curr[3],4), ")", sep=""))
print(paste("Estimate p = (", round(EM2.more$p.curr[1],4), ",",
round(EM2.more$p.curr[2],4), ",",
round(EM2.more$p.curr[3],4), ")", sep=""))
EM2.more$log_liks[length(EM2.more$log_liks)]
all(diff(EM2.more$log_liks) > 0)
plot(EM2.more$log_liks, ylab='incomplete log-likelihood', xlab='iteration')
data <- read.csv(file = "property_final.csv",header = TRUE)
setwd("~/ADS/P2/generic-real-estate-consulting-project-group-6/scripts")
setwd("~/")
setwd("~/")
data <- read.csv(file = "../data/curated/property_final.csv",header = TRUE)
setwd("~/ADS/P2/generic-real-estate-consulting-project-group-6/scripts")
data <- read.csv(file = "../data/curated/property_final.csv",header = TRUE)
data$cloest.school<-factor(data$cloest.school)
data$type <- factor(data$type)
data$cloest.station <- factor(data$cloest.station)
str(data)
# pair plot for numerical features
library(GGally)
ggpairs(data, columns=c(4,6,7,10,11,12,13,16,17,18,19))+ggtitle("pairplot")+theme_bw()
m1 <- glm(
cost ~ type+station_distance+CBD_distance+beds+bath+parking+suburb_population+density+offence_count_scaled+X2022_income,
data=data,
family = gaussian(link = "identity")
)
summary(m1)
library("DALEX")
library("ingredients")
explain_titanic_glm <- explain(m1,data=data[,-c(1,3,5,8,9,10,14,15)],y = data[,10])
fig<- feature_importance(explain_titanic_glm, B = 1)
plot(fig)
library(glmnet)
m2 <- glmnet(data[,-c(1,2,3,5,8,9,10,22,12,13,14,15)], y = data[,10],alpha = 1, family = 'gaussian')
summary(m2)
x <- as.matrix(data[,-c(1,2,3,5,8,9,10,14,15)])
y <- as.matrix(data[,10])
f1 = glmnet(x, y, family="gaussian", nlambda=20, alpha=1)
print(f1)
plot(f1, xvar="lambda", label=TRUE)
cvfit=cv.glmnet(x,y,family = 'gaussian')
plot(cvfit)
cvfit$lambda.min
cvfit$lambda.1se
l.coef2<-coef(cvfit$glmnet.fit,s=0.245865,exact = F)
l.coef1<-coef(cvfit$glmnet.fit,s=28.2685,exact = F)
l.coef1
l.coef2
m1 <- glm(
cost ~ beds+bath+density+X2022_income,
data=data,
family = gaussian(link = "identity")
)
summary(m1)
# pair plot for numerical features
library(GGally)
ggpairs(data, columns=c(4,6,7,10,11,12,13,16,17,18,19))+ggtitle("pairplot")+theme_bw()
library("DALEX")
library("ingredients")
explain_titanic_glm <- explain(m1,data=data[,-c(1,3,5,8,9,10,14,15)],y = data[,10])
fig<- feature_importance(explain_titanic_glm, B = 1)
library(glmnet)
m2 <- glmnet(data[,-c(1,2,3,5,8,9,10,22,12,13,14,15)], y = data[,10],alpha = 1, family = 'gaussian')
summary(m2)
x <- as.matrix(data[,-c(1,2,3,5,8,9,10,14,15)])
y <- as.matrix(data[,10])
f1 = glmnet(x, y, family="gaussian", nlambda=20, alpha=1)
print(f1)
plot(f1, xvar="lambda", label=TRUE)
cvfit=cv.glmnet(x,y,family = 'gaussian')
cvfit$lambda.min
cvfit$lambda.1se
l.coef2<-coef(cvfit$glmnet.fit,s=0.245865,exact = F)
l.coef1<-coef(cvfit$glmnet.fit,s=28.2685,exact = F)
l.coef1
l.coef2
tinytex::install_tinytex
devtools::install_github(c('rstudio/rmarkdown', 'yihui/tinytex'))
tinytex::install_tinytex
devtools::install_github(c('rstudio/rmarkdown', 'yihui/tinytex'))
#Model used: GLM
Generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
```{r}
data <- read.csv(file = "../data/curated/property_final.csv",header = TRUE)
data$cloest.school<-factor(data$cloest.school)
data$type <- factor(data$type)
data$cloest.station <- factor(data$cloest.station)
str(data)
data <- read.csv(file = "../data/curated/property_final.csv",header = TRUE)
data$cloest.school<-factor(data$cloest.school)
data$type <- factor(data$type)
data$cloest.station <- factor(data$cloest.station)
str(data)
# Assumption:
1.Linear relationship:According to the pair plot above, it can be seen that there is a potential linear relationship between cost and other features.
# regularization
In the glmnet package, Lasso or Ridge is considered for the regression model of multi-dimensional features. However, the parameter β of ridge will not be equal to 0, but the parameter β of Lasso can be equal to 0, so Lasso is chosen.
install.packages(‘devtools’)
install.packages("devtools")
#Model used: GLM
Generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
devtools::install_github(c('rstudio/rmarkdown', 'yihui/tinytex'))
devtools::install_github(c('rstudio/rmarkdown', 'yihui/tinytex'))
tinytex::install_tinytex()
devtools::install_github(c('rstudio/rmarkdown', 'yihui/tinytex'))
tinytex::install_tinytex()
data <- read.csv(file = "property_final.csv",header = TRUE)
data <- read.csv(file = "property_final.csv",header = TRUE)
setwd("~/ADS/P2/generic-real-estate-consulting-project-group-6/scripts")
data <- read.csv(file = "../data/curated/property_final.csv",header = TRUE)
data$cloest.school<-factor(data$cloest.school)
data$type <- factor(data$type)
data$cloest.station <- factor(data$cloest.station)
str(data)
# pair plot for numerical features
library(GGally)
ggpairs(data, columns=c(4,6,7,10,11,12,13,16,17,18,19))+ggtitle("pairplot")+theme_bw()
m1 <- glm(
cost ~ type+station_distance+CBD_distance+beds+bath+parking+suburb_population+density+offence_count_scaled+X2022_income,
data=data,
family = gaussian(link = "identity")
)
summary(m1)
library("DALEX")
library("ingredients")
explain_titanic_glm <- explain(m1,data=data[,-c(1,3,5,8,9,10,14,15)],y = data[,10])
fig<- feature_importance(explain_titanic_glm, B = 1)
fig<- feature_importance(explain_titanic_glm, B = 1)
plot(fig)
library(glmnet)
m2 <- glmnet(data[,-c(1,2,3,5,8,9,10,22,12,13,14,15)], y = data[,10],alpha = 1, family = 'gaussian')
summary(m2)
x <- as.matrix(data[,-c(1,2,3,5,8,9,10,14,15)])
y <- as.matrix(data[,10])
f1 = glmnet(x, y, family="gaussian", nlambda=20, alpha=1)
print(f1)
plot(f1, xvar="lambda", label=TRUE)
cvfit=cv.glmnet(x,y,family = 'gaussian')
plot(cvfit)
cvfit$lambda.min
cvfit$lambda.1se
l.coef2<-coef(cvfit$glmnet.fit,s=0.245865,exact = F)
l.coef1<-coef(cvfit$glmnet.fit,s=28.2685,exact = F)
l.coef1
l.coef2
m1 <- glm(
cost ~ beds+bath+density+X2022_income,
data=data,
family = gaussian(link = "identity")
)
summary(m1)
data.pair
data.pair()
pair(data)
pairs(data)
pairs(data[:,c(4,6,7,10,11,12,13,16,17,18,19)])
pairs(dat[,c(4,6,7,10,11,12,13,16,17,18,19)])
pairs(dat[:,c(4,6,7,10,11,12,13,16,17,18,19)])
pairs(data[:,c(4,6,7,10,11,12,13,16,17,18,19)])
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)])
# pair plot for numerical features
# library(GGally)
# ggpairs(data, columns=c(4,6,7,10,11,12,13,16,17,18,19))+ggtitle("pairplot")+theme_bw()
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)], size=(10,10))
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)], figsize=(10,10))
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)], pch = 18)
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)], pch = 2)
pairs(data[c(4,6,7,10,11,12,13,16,17,18,19)], pch = 46)
